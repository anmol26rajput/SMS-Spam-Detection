{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SMS Spam Detection","metadata":{}},{"cell_type":"markdown","source":"The file we are going to use contains a collection of more than 5 thousand SMS phone messages. Using labeled ham and spam examples, we'll **train a machine learning model to learn to discriminate between ham/spam automatically**. Then, with a trained model, we'll be able to **classify arbitrary unlabeled messages** as ham or spam.\n\nHere I  am going to develop an SMS spam detector using **SciKit Learn's Naive Bayes classifier algorithm**. However before feeding data to Machine Learning NB algorithim, we need to process each SMS with the help of Natural Language libraries.","metadata":{}},{"cell_type":"markdown","source":"# Summary of building the model\n\nLet me give you a brief idea that I am going to follow in this notebook to create the model:\n\n\n* First try to understand the data and its distribution with basic EDA with the help of Pandas and Matplotlib libraries. Also, check for any outliers by analysing the distribution graphs. \n\n* Now with the help of NLP library **\"NLTK\"**, first **remove the punctuation** and **special symbols** from all the SMS and then **lower case** them. You can even **tokenize** each SMS into sentences and words after removing punctuation & special symbols. Here I am just splitting each SMS into words with white spaces. However, tokenization and parsing may be the best idea to split the texts. Please note that converting all the data to lower case helps in the process of preprocessing and in later stages in the NLP application.\n\n* Then remove the **Stopswords** from all the SMS.\n\n* After processing each SMS, we will create the **WordCloud** for Spam and Ham messages for the visual representation of widely used words in both Spam and Ham messages.\n\n* Now we can normalize the text by NLTK **lemmatization** or **stemming** or distinguishing by **part of speech (POC)**. However, sometimes these methods don't work well especially for text-messages due to the way a lot of people tend to use abbreviations or shorthand in SMS. E.g. \"IDK\" for \"I don't know\" or \"wut\" for \"what\". So we will not process the text by these methods.\n\n* For now, we will have the messages as lists of tokens and now we need to convert each of these messages into a vector so that SciKit Learn's algorithm models can work with.\n\n    We'll do that in three steps using the **bag-of-words (BOW)** model:\n    \n\n    * Count how many times does a word occur in each message (Known as term frequency - **TF**)\n    \n    * Weigh the counts, so that frequent tokens get lower weight (inverse document frequency - **IDF**)\n    \n    * Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n    \n\n\n* Once the messages represented as vectors, we can finally train our spam/ham classifier. Now we can actually use almost any sort of **classification algorithms** like Random Forest, Naive Bayes etc.","metadata":{}},{"cell_type":"markdown","source":"## Natural Language Processing (NLP)\n\nHere the messages are in the human-readable language which computer can't understand, so we have to use the NLP to make it possible for computers to read human (natural) language SMS and determine which parts are important.\n\nSo, Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language.\n\nNLP makes it possible for computers to read the text, hear speech, interpret it, measure sentiment and determine which parts are important.\n\nThe input and output of an NLP system can be −\n* Speech\n* Written Text\n\nLearn **basics of NLP** from [my GitHub code](https://github.com/dktalaicha/Natural-Language-Processing/blob/master/Natural-Language-Processing-Basics.ipynb). This will help to understand the jargons of NLP.","metadata":{}},{"cell_type":"markdown","source":"# Load the Data\n\nLets import the Python libraries first and then the file through pandas to get a list of all the lines of text messages:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n%matplotlib inline\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',encoding = 'latin-1')\nmessages.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove the unnecessary columns for dataset and rename the column names.","metadata":{}},{"cell_type":"code","source":"messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\nmessages.columns = [\"label\", \"message\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nLet's check out some of the stats with some plots and the built-in methods in pandas!","metadata":{}},{"cell_type":"code","source":"messages.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are total 5572 SMS in this dataset with 2 columns label and message.","metadata":{}},{"cell_type":"code","source":"messages.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* There is two unique labels.\n* There are some repeated messages as unique is less that the count due to some comman messages.","metadata":{}},{"cell_type":"markdown","source":"Let's use **groupby** to use describe by label, this way we can begin to think about the features that separate ham and spam!","metadata":{}},{"cell_type":"code","source":"messages.groupby('label').describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 4825 ham messages out of which 4516 are unique..\n* 747 span messages out of which 653 are unique.\n* \"Sorry, I'll call later\" is the most popular ham message with repetition of 30 times.\n* \"Please call our customer service representativ...\" is the most popular spam message with repetition 4 times.","metadata":{}},{"cell_type":"markdown","source":"As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the general idea of **feature engineering**. The better the domain knowledge, better the ability to engineer more features from it. \n\n**Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.**\n\nLet's make a new feature to detect how long the text messages are:","metadata":{}},{"cell_type":"code","source":"messages['length'] = messages['message'].apply(len)\nmessages.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the frequency of top 5 messages.\nmessages['message'].value_counts().rename_axis(['message']).reset_index(name='counts').head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems people are really busy. \"Sorry, i'll call later\" tops the message list with 30 counts with \"I cant pick the phone right now. Pls send a message\" comes second with 12 counts.\n\nTheres a quite lot of Ok...Okie. in there too.","metadata":{}},{"cell_type":"markdown","source":"## Data Visualization\nLet's visualize this! ","metadata":{}},{"cell_type":"code","source":"messages[\"label\"].value_counts().plot(kind = 'pie',explode=[0, 0.1],figsize=(6, 6),autopct='%1.1f%%',shadow=True)\nplt.title(\"Spam vs Ham\")\nplt.legend([\"Ham\", \"Spam\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A lot of messages are actually not spam. About 86% of our dataset consists of normal messages.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nmessages['length'].plot(bins=100, kind='hist') # with 100 length bins (100 length intervals) \nplt.title(\"Frequency Distribution of Message Length\")\nplt.xlabel(\"Length\")\nplt.ylabel(\"Frequency\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the message lenght is less than 200. Note that x-axis goes all the way to 1000ish, this must mean that there is some really long message!","metadata":{}},{"cell_type":"code","source":"messages['length'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Woah! 910 characters, let's use masking to find this message:","metadata":{}},{"cell_type":"code","source":"messages[messages['length'] == 910]['message'].iloc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we have some sort of Romeo sending texts! But let's focus back on the idea of trying to see if message length is a distinguishing feature between ham and spam:","metadata":{}},{"cell_type":"code","source":"messages.hist(column='length', by='label', bins=50,figsize=(12,4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like spam messages are usually longer. Maybe message length can become a feature to predict whether the message is spam/ ham ?\n\nNow let's begin to process the data so we can eventually use it with SciKit Learn!","metadata":{}},{"cell_type":"markdown","source":"# Text Pre-processing","metadata":{}},{"cell_type":"markdown","source":"Our main issue with our data is that it is all in text format (strings). The classification algorithms will need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a **corpus** to a vector format. The simplest is the the [bag-of-words](https://github.com/dktalaicha/Natural-Language-Processing/blob/master/Natural-Language-Processing-Basics.ipynb) approach, where each unique word in a text will be represented by one number.\n\nIn this section we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers).\n\nAs a first step, let's write a function that will split a message into its individual words and return a list. We'll also remove very common words i.e. **stopwords**, ('the', 'a', etc..). To do this we will take advantage of the NLTK library.\n\nLet's create a function that will process the string in the message column, then we can just use **apply()** in pandas do process all the text in the DataFrame.\n\nFirst removing punctuation. We can just take advantage of Python's built-in **string** library to get a quick list of all the possible punctuation:","metadata":{}},{"cell_type":"code","source":"def text_preprocess(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    nopunc = nopunc.lower()\n    \n    # Now just remove any stopwords and non alphabets\n    nostop=[word for word in nopunc.split() if word.lower() not in stopwords.words('english') and word.isalpha()]\n    \n    return nostop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's \"tokenize\" these spam or ham messages. Tokenization is just the term used to describe the process of converting the normal text strings in to a list of tokens (words that we actually want).\n\nLet's see an example output on on column:\n\n<div class=\"alert alert-block alert-warning\">\n<b>Note:</b> \nWe may get some warnings or errors for symbols we didn't account for or that weren't in Unicode (like a British pound symbol)\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Lets study individual spam/ham messages","metadata":{}},{"cell_type":"code","source":"spam_messages = messages[messages[\"label\"] == \"spam\"][\"message\"]\nham_messages = messages[messages[\"label\"] == \"ham\"][\"message\"]\nprint(\"No of spam messages : \",len(spam_messages))\nprint(\"No of ham messages : \",len(ham_messages))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordcloud for Spam Messages","metadata":{}},{"cell_type":"code","source":"# This may take a while....\nspam_words = text_preprocess(spam_messages)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets print some spam words\nspam_words[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam_wordcloud = WordCloud(width=600, height=400).generate(' '.join(spam_words))\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wordcloud for spam messages shows that words like call, txt, win, free, reply, mobile, text etc. are widely used, let's check them statistically.**","metadata":{}},{"cell_type":"code","source":"print(\"Top 10 Spam words are :\\n\")\nprint(pd.Series(spam_words).value_counts().head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordcloud for Ham Messages","metadata":{}},{"cell_type":"code","source":"# This may take a while...\nham_words = text_preprocess(ham_messages)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets pring some ham words\nham_words[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ham_wordcloud = WordCloud(width=600, height=400).generate(' '.join(ham_words))\nplt.figure( figsize=(10,8), facecolor='k')\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wordcloud for ham messages shows that words like got, come, go, ur, know, call etc. are widely used, let's check them statistically.**","metadata":{}},{"cell_type":"code","source":"print(\"Top 10 Ham words are :\\n\")\nprint(pd.Series(ham_words).value_counts().head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Transformation\n\nLets clean our data by removing punctuations/ stopwords.","metadata":{}},{"cell_type":"code","source":"messages.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This might take a while....\n\n# Lets remove punctuations/ stopwords from all SMS \nmessages[\"message\"] = messages[\"message\"].apply(text_preprocess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conver the SMS into string from list\nmessages[\"message\"] = messages[\"message\"].agg(lambda x: ' '.join(map(str, x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages[\"message\"][7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Continuing Normalization\n\nThere are a lot of ways to continue normalizing the text. Such as **Stemming** or distinguishing by **part of speech**.\n\nNLTK has lots of built-in tools. However sometimes they don't work well for text-messages due to the way a lot of people tend to use abbreviations or shorthand, For example:\n    \n    'Nah dawg, IDK! Wut time u headin to da club?'\n    \nversus\n\n    'No dog, I don't know! What time are you heading to the club?'\n    \nSome text normalization methods will have trouble with this type of shorthand and so we are not going to use them here. For now we will just focus on using what we have to convert our list of words to an actual vector that SciKit-Learn can use.","metadata":{}},{"cell_type":"markdown","source":"## Vectorization\n\nCurrently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of these messages into a vector the SciKit Learn's algorithm models can work with.\n\nNow we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n\nWe'll do that in three steps using the **bag-of-words** model:\n\n1. Count how many times does a word occur in each message (Known as term frequency or **TF**)\n\n2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency or **IDF**)\n\n3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n\nLet's begin the first step:\n\nEach vector will have as many dimensions as there are unique words in the SMS corpus.  We will first use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n\nCountvectorizer counts the number of times a token shows up in the messages and uses this value as its weight.\n\nWe can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message. \n\nSince there are so many messages, we can expect a lot of zero counts for the presence of that word in that document. Because of this, SciKit Learn will output a Sparse Matrix.","metadata":{}},{"cell_type":"markdown","source":"## Creating the Bag of Words","metadata":{}},{"cell_type":"code","source":"# Creating the Bag of Words\n\n# Note the here we are passing already process messages (after removing punctuations and stopwords)\n\nvectorizer = CountVectorizer()\nbow_transformer = vectorizer.fit(messages['message'])\n\nprint(\"20 Bag of Words (BOW) Features: \\n\")\nprint(vectorizer.get_feature_names()[20:40])\n\nprint(\"\\nTotal number of vocab words : \",len(vectorizer.vocabulary_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take one text message and get its bag-of-words counts as a vector, putting to use our new `bow_transformer`:","metadata":{}},{"cell_type":"code","source":"message4 = messages['message'][3]\nprint(message4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see its vector representation:","metadata":{}},{"cell_type":"code","source":"# fit_transform : Learn the vocabulary dictionary and return term-document matrix.\nbow4 = bow_transformer.transform([message4])\nprint(bow4)\nprint(bow4.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This means that there are seven unique words in message number 4 (after removing common stop words). Let's go ahead and check and confirm which ones appear twice:","metadata":{}},{"cell_type":"code","source":"print(bow_transformer.get_feature_names()[5945])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now we can use **.transform** on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of messages. Let's go ahead and check out how the bag-of-words counts for the entire SMS corpus is a large, sparse matrix:","metadata":{}},{"cell_type":"code","source":"messages_bow = bow_transformer.transform(messages['message'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Shape of Sparse Matrix: ', messages_bow.shape)\nprint('Amount of Non-Zero occurences: ', messages_bow.nnz)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the counting, the term weighting and normalization can be done with TF-IDF.","metadata":{}},{"cell_type":"markdown","source":"### TF-IDF\n\nTF-IDF stands for **term frequency-inverse document frequency**, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n\n\n**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n\n$$ TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}. $$\n   \n\n**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n\n$$ IDF(t) = \\log_e\\Bigg( \\frac{\\text{Total number of documents}}{\\text{Number of documents with term t in it}} \\Bigg). $$\n\nLet's do this in SciKit Learn.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(messages_bow)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try classifying our single random message and checking how we do:","metadata":{}},{"cell_type":"code","source":"tfidf4 = tfidf_transformer.transform(bow4)\nprint(tfidf4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(bow_transformer.get_feature_names()[5945])\nprint(bow_transformer.get_feature_names()[3141])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll go ahead and check what is the IDF (inverse document frequency) of the `\"say\"`?","metadata":{}},{"cell_type":"code","source":"print(tfidf_transformer.idf_[bow_transformer.vocabulary_['say']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### To transform the entire bag-of-words corpus into TF-IDF corpus at once:","metadata":{}},{"cell_type":"code","source":"messages_tfidf = tfidf_transformer.transform(messages_bow)\nprint(messages_tfidf.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"messages[\"message\"][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets convert our clean text into a representation that a machine learning model can understand. I'll use the Tfifd for this.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvec = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", stop_words = \"english\")\nfeatures = vec.fit_transform(messages[\"message\"])\nprint(features.shape)\n\nprint(len(vec.vocabulary_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are many ways the data can be preprocessed and vectorized. These steps involve feature engineering and building a \"pipeline\". I encourage you to check out SciKit Learn's documentation on dealing with text data as well as the expansive collection of available papers and books on the general topic of NLP.","metadata":{}},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"With messages represented as vectors, we can finally train our spam/ham classifier. Now we can actually use almost any sort of classification algorithms. For a variety of reasons, the Naive Bayes classifier algorithm is a good choice.","metadata":{}},{"cell_type":"markdown","source":"We'll be using scikit-learn here, choosing the Naive Bayes classifier to start with:","metadata":{}},{"cell_type":"markdown","source":"#### Encoding class labels in Target Variable (Not Mandatory)\n\nEncoding the Target variable (here label as spam or ham) into numeric form is ideal before data is fed to Machine Learning algorithms. I am just escaping this now as Naive Bayes can work with categorial target variable. ","metadata":{}},{"cell_type":"code","source":"# FactorResult=pd.factorize(messages['label'])\n# messages['label'] = FactorResult[0]\n# messages.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"msg_train, msg_test, label_train, label_test = \\\ntrain_test_split(messages_tfidf, messages['label'], test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train dataset features size : \",msg_train.shape)\nprint(\"train dataset label size\", label_train.shape)\n\nprint(\"\\n\")\n\nprint(\"test dataset features size\", msg_test.shape)\nprint(\"test dataset lable size\", label_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test size is 20% of the entire dataset (1115 messages out of total 5572), and the training is the rest (4457 out of 5572). Note the default split would have been 30/70.","metadata":{}},{"cell_type":"markdown","source":"## Building Naive Bayes classifier Model\n\nLet's create a Naive Bayes classifier Model using Scikit-learn.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()\nspam_detect_model = clf.fit(msg_train, label_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_train = spam_detect_model.predict(msg_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Classification Report \\n\",metrics.classification_report(label_train, predict_train))\nprint(\"\\n\")\nprint(\"Confusion Matrix \\n\",metrics.confusion_matrix(label_train, predict_train))\nprint(\"\\n\")\nprint(\"Accuracy of Train dataset : {0:0.3f}\".format(metrics.accuracy_score(label_train, predict_train)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try classifying our single random message and checking how we do:","metadata":{}},{"cell_type":"code","source":"print('predicted:', spam_detect_model.predict(tfidf4)[0])\nprint('expected:', messages['label'][3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fantastic! We've developed a model that can attempt to predict spam vs ham classification!","metadata":{}},{"cell_type":"markdown","source":"## Model Evaluation\nNow we want to determine how well our model will do overall on the entire dataset. Let's begin by getting all the predictions:","metadata":{}},{"cell_type":"code","source":"label_predictions = spam_detect_model.predict(msg_test)\nprint(label_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use SciKit Learn's built-in classification report, which returns precision, recall f1-score, and a column for support (meaning how many cases supported that classification).","metadata":{}},{"cell_type":"code","source":"print(metrics.classification_report(label_test, label_predictions))\nprint(metrics.confusion_matrix(label_test, label_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are quite a few possible metrics for evaluating model performance. Which one is the most important depends on the task and the business effects of decisions based off of the model. For example, the cost of mis-predicting \"spam\" as \"ham\" is probably much lower than mis-predicting \"ham\" as \"spam\".","metadata":{}},{"cell_type":"code","source":"# Printing the Overall Accuracy of the model\nprint(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(label_test, label_predictions)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I welcome comments, suggestions, corrections and of course votes also.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}